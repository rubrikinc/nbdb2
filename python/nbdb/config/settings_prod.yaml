# Sparse store related settings
sparse_store:
  # Number of data point keys in a single partition
  partition_size: 1000
  druid_mode: true
  ttl_days: 365
  # If true we transform the data by applying some streaming functions like non-negative-derivatives
  # this is not prod ready and meant for running benchmarks to estimate the value
  enable_stream_transforms: false

  # Rollup windows
  # rollup of 3600, means all points within 1 hr (aligned to epoch 0 at hourly boundary) are aggregated
  # based on the rollup function defined in schema file
  rollups:
    windows: [3600, 14400]
    report_clock_skew: true

  sparse_algos:
    min_delta: 0.05
    # Number of seconds after which a data point is forced to be written even if it has not changed
    forced_write_interval: 43200
    quantile: .9
    window_size: 10

  sparse_telemetry:
    # Log metrics which are dropping less than this threshold.
    # Range: 0.0 to 1.0
    drop_ratio_log_threshold: 0
    gen_sparse_data_distribution: false

  # Stats related settings
  stats:
    # How many stats objects are cached in memory
    cache_size: 50000000
    # triggers an expensive byte size scan this will impact throughput, should be done sparingly
    # ideally disabled on production and maybe enabled on one metric consumer to get an idea
    telemetry_report_interval_seconds: 1800

  # Settings for heartbeat scan algorithm that detects missing data points and marks them with a tombstone
  heartbeat_scan:
    interval: 600
    # A missing point marker is added if the current datapoint is not within
    # `data_gap_detection_interval` seconds of the previous datapoint
    data_gap_detection_interval: 600
    # A series is terminated and a tombstone marker is added if no datapoints
    # are seen within `termination_detection_interval` seconds
    termination_detection_interval: 3600

tracing_config:
  s3_bucket: "anomalydb-prod-druid"
  s3_key: "tracing_config.yaml"
  # We check S3 ever `refresh_interval` seconds to see if a new config was
  # uploaded
  refresh_interval: 600

# Kafka implementation of StoreInterface (we only support write functionality on data points)
# The sparse data points are written to this kafka stream by the DataPointsBatchWriter
# Druid can directly consume some topics from this kafka stream
# other consumers (rollups for example) or baseline detection can also consume sparse data directly
sparse_kafka:
  # Broker address
  connection_string:
    batch_size: 10000
    bootstrap:
      servers: "<ADD_KAFKA_BROKER_LIST>"
    enable:
      idempotence: false
    queue:
      buffering:
        max:
          messages: 1000000
          kbytes: 1048576
          ms: 1000
    message:
      send:
        max:
          retries: 2
      timeout:
        ms: 300000
    compression:
      codec: 'snappy'
    delivery:
      report:
        only:
          error: true
    request:
      required:
        acks: 1
  # Number of replicas per topic (created for each measurement)
  replicas: 3
  # Number of partitions per topic (created for each measurement)
  partitions: 6
  # time in ms for how long the topics data is retained in memory
  retention: 43200000
  # time in ms for how long the topics data is retained on the disk
  segment: 43200000

Druid:
  connection_string:
    overlord_ip: "<ADD_OVERLORD_IP>"
    overlord_port: 8090
    router_ip: "<ADD_ROUTER_IP>"
    router_port: 80
    cc_router_ip: "<ADD_CC_ROUTER_IP>"
    cc_router_port: 80
    read_path: '/druid/v2/sql/'
    scheme: 'http'
    # Interval in seconds after which druidadmin reloads the full schema in memory
    schema_refresh_time: 600
  datasource:
    maxRowsInMemory: 100000
    maxRowsPerSegment: 40000000
    # Drop messages that are older than 1 month. Since our regular datasources
    # only retain data for the last month, it doesn't make sense to store older
    # datapoints only for them to get deleted immediately.
    # NOTE: This will affect any historical metrics usecases
    lateMessageRejectionPeriod: P30D
    # Drop messages which are too early. Ideally this should never happen
    earlyMessageRejectionPeriod: P2D
  batch_ingest:
    # From https://druid.apache.org/docs/latest/ingestion/native-batch.html,
    # If this value is set to 1, the supervisor task processes data ingestion on
    # its own instead of spawning worker tasks.
    maxNumConcurrentSubTasks: 2

schema_creator:
  # number of workers used by the schema_creator to periodically create the schema
  num_workers: 1
  cleanup_on_boot: false
  druid_topic_prefix: "anomalydb_druid"

# Prod metric consumer settings. Reads all metrics written to anomalydb.live.rubrik.com:80
metric_consumer:
  kafka_brokers: "<ADD_BROKER_LIST>"
  topic: "test-master-line-metrics"
  group_prefix: "anomalydb_metric_consumers"
  num_consumers: 1
  metric_protocol: influx
  # We flush our writes every 60 secs
  flush_writes_period_secs: 60
  whitelist_cluster_uuid:
  # Messages with timestamp older than this will be logged
  past_message_lag: 10800
  # Messages with timestamp newer than this will be logged
  future_message_lag: 3600

  redis_host: "<ADD_REDIS_HOST>"
  redis_port: 6379

recovery_consumer:
  num_consumers: 1

sparse_batch_consumer:
  kafka_brokers: # Omitted since we are not enabling yet
  topic: "anomalydb_sparse_batch_consumer"
  group: "anomaly_db_sparse_batch_consumers_group"
  metric_protocol: graphite
  num_consumers: 5
  max_inflight_tasks: 10
  max_inflight_messages: 100
  max_messages_to_combine: 5
  # Fetch tasks for the last N days on consumer assignment
  num_days_to_fetch_on_assign: 2
  # Bucket to fetch sparse JSONs from
  s3_bucket: anomalydb-batch-ingest

dense_batch_consumer:
  kafka_brokers: # Omitted since we are not enabling yet
  topic: "anomalydb_dense_batch_consumer"
  group: "anomaly_db_dense_batch_consumers_group"
  num_consumers: 1
  s3_bucket: jarvis-metrics-lake
  # Messages with timestamp older than this will be reported through clock
  # skew. Turning on debug logging will log all such messages
  past_message_lag: 2592000  # 30d
  # Messages with timestamp newer than this will be reported through clock
  # skew. Turning on debug logging will log all such messages
  future_message_lag: 172800  # 2d
  # Only convert the following cluster UUIDs. If empty, all clusters are
  # converted
  conversion_whitelist:
    - "<WHITELIST_CLUSTER>"

# Settings for generating test metrics and writing to kafka
test_metric_producer:
  kafka_brokers: ["<BROKER1:PORT1>",
                  "<BROKER2:PORT2>",
                  "<BROKER3:PORT3>"]
  topic: "test-master-line-metrics"
  # We map the node-id to partition by using node%num_partitions
  # Note the num_partitions must match the value used to create the topic
  num_partitions: 50
  repl_factor: 3
  # After writing all series for one epoch, the producer will sleep for throttle_time
  throttle_time: 0
  # For each series generate num_epochs data points with epoch values in range
  # [epoch_start, epoch_end]
  # epoch_start: 10
  # epoch_end: 100
  last_hours: 1
  interval_seconds: 60
  live: true
  # A series is per node and per process, so total series = num_nodes*num_processes
  # num_clusters per TMP, the cluster_ids are in range(cluster_id_command_line, cluster_id_command_line + num_clusters)
  num_clusters: 4
  # number of nodes per cluster
  num_nodes: 10
  # The value of series ranges between [min_value, max_value]
  min_value: 0
  max_value: 5000
  # The series values are generated using a random walk, where:
  #  next value = current_value + random(min_change, max_change)
  min_change: -10
  max_change: 10
  use_random_walk: true
  # Parameters for the metric being generated
  cluster_tag_key: "cluster"
  cluster_tag_value_prefix: "<CLUSTER_PREFIX>"
  node_tag_key: "node"
  node_tag_value_prefix: "<NODE_PREFIX>"
  # Measurement names starting with this prefix in schema file are assumed to be generated by the test metric producer
  metric_prefix: 'measurement_really_long_name_'
  field_prefix: 'field_'
  num_fields: 100

# Settings for reading test metrics
test_dashboards:
  # We generate measurements*fields api calls
  # In real world a dashboard has panels, and each panel has queries and each query has fields
  # we model a dashboard with 10 panels, each panel with 5 queries and each query with 1 field
  # so total number of api calls / dashboard = 10*5 = 50
  # number of fields must match the schema_prod.yaml
  fields: 10
  # we create a dashboard per cluster
  cluster_start: 1
  cluster_max: 300
  cluster_inc_step: 1
  cluster_inc_after_iterations: 1
  dashboards_per_cluster: 1
  measurement_prefix: "measurement_really_long_name_"
  field_prefix: "field_"
  cluster_prefix: "<CLUSTER_PREFIX>"
  required_lag: 600
  last_hours: 24
  points_per_query: 1440
  refresh_points: 1

test_metric_reader:
  # After running one query, the reader will sleep for this much time in secs
  throttle_time: 0

  # Settings used for parallelizing queries
  num_workers: 1000
  max_pending_futures: 20

  # Benchmark type. We support two types:
  # 1. simple_queries: Queries which need no aggregation
  # 2. agg_queries: Queries which require aggregation
  # 3. agg_downsampled_queries: Queries which require aggregation as well as
  # downsampling
  benchmark_type: simple_queries
  # Downsampling interval. Only required for `agg_downsampled_queries'
  # benchmarks
  downsampling_interval_mins:
  # Throttling settings
  rate_limit: 400
  sleep_time: 0.1
  reset_time: 5

  # Epoch ranges to be used while querying. These should be smaller or equal to
  # those used by test_metric_producer
  start_epoch: 1584673110
  end_epoch: 1584676721
  # These node & process ranges should match those used by test_metric_producer
  num_nodes: 20
  num_processes: 4
  flask_addr: "anomaly.rubrik.com"

  # Name of the metric field
  measurement: "t_a1"
  field_name: "f_1"
  node_tag_key: "n"
  process_tag_key: "p"

sql_api:
  # Compute bound workers that execute functions on data returned from store, there is no I/O on these threads
  workers: 50
  # These workers execute queries against store and are mostly I/O bound
  store_read_workers: 500
  # These workers run queries against the indexing backend and are mostly I/O
  # bound
  index_search_workers: 10
  default_group_by_interval: 60
  query_cache_size: 100000
  # Number of points per series per interval to cache
  #
  # Default behaviour is to cache 1008 datapoints.  10m granularity means 144
  # datapoints per day for a single series. A limit of 1008 datapoints means 7
  # days worth of datapoints. Exceeding this risks reaching the max String
  # value limit of 512 MB especially for cross-cluster queries.
  points_per_series:
    default: 1008
    # SentryAI queries only the latest 30m data. It doesn't make sense to cache
    # more than 1h worth of data because it will never be requeried.
    # Additionally since SentryAI only issues cross-cluster queries, caching
    # 7d worth of datapoints ends up crossing the 512 MB Redis string value
    # limit sometimes. By setting to 1h, we guarantee that it will never happen
    anomalydb_sentryai: 6
    # Alert Framework also uses cross-cluster queries, but the query window
    # varies based on the alert definition. We cache results for 24h to cover
    # most cases
    api_key: 144
  cache_provider: "redis"
  host: "<REDIS_HOST>"
  port: 6379
  db_id: 1
  ttl_seconds: 172800

flask:
  port: 5000
  host: "0.0.0.0"
  enable_cache: true

logging:
  # Set to true (recommended for containers) to only log to console, false if log to file
  log_to_console: true
  # Log per metric level details (Extremely verbose, meant only for debugging)
  log_metrics: false
  # must be specified when log_to_console is false
  # log_dir: "/mnt/anomalydb/log"
  debug: false

telemetry:
  protocol: influx
  # We add the environment tag to all metrics
  environment: production
  influx:
    # Interval in seconds at which frequency the metrics are pushed to graphite
    reporting_interval: 600
    prefix: "anomalydb"
    database: "anomalydb-metrics"
    server: "<INFLUX_SERVER>"
    port: 80
