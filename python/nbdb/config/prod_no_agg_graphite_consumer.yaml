# Sparse store related settings
sparse_store:
  # Number of data point keys in a single partition
  partition_size: 1000
  druid_mode: true
  ttl_days: 365
  # If true we transform the data by applying some streaming functions like non-negative-derivatives
  # this is not prod ready and meant for running benchmarks to estimate the value
  enable_stream_transforms: false

  rollups:
    report_clock_skew: true

  sparse_algos:
    min_delta: 0.05
    # Number of seconds after which a data point is forced to be written even if it has not changed
    forced_write_interval: 43200
    quantile: .9
    window_size: 10

  sparse_telemetry:
    # Log metrics which are dropping less than this threshold.
    # Range: 0.0 to 1.0
    drop_ratio_log_threshold: 0
    gen_sparse_data_distribution: false

  # Stats related settings
  stats:
    # How many stats objects are cached in memory
    cache_size: 10000000
    # Triggers an expensive byte size scan this will impact throughput. Should
    # be done sparingly ideally disabled on production and maybe enabled on one
    # metric consumer to get an idea
    telemetry_report_interval_seconds: 300

  # Settings for heartbeat scan algorithm that detects missing data points and
  # marks them with a tombstone
  heartbeat_scan:
    interval: 1800
    # A missing point marker is added if the current datapoint is not within
    # `data_gap_detection_interval` seconds of the previous datapoint
    data_gap_detection_interval: 600
    # A series is terminated and a tombstone marker is added if no datapoints
    # are seen within `termination_detection_interval` seconds
    termination_detection_interval: 3600

# We allow the ability to "trace" certain metrics. The tracing config is
# periodically pulled from an S3 location
tracing_config:
  s3_bucket: "anomalydb-prod-druid"
  s3_key: "tracing_config.yaml"
  # We check S3 ever `refresh_interval` seconds to see if a new config was
  # uploaded
  refresh_interval: 600

# Kafka implementation of StoreInterface (we only support write functionality on data points)
# The sparse data points are written to this kafka stream by the DataPointsBatchWriter
# Druid can directly consume some topics from this kafka stream
# other consumers (rollups for example) or baseline detection can also consume sparse data directly
sparse_kafka:
  # Broker address
  connection_string:
    batch_size: 10000
    bootstrap:
      servers: "b-1.kafka.com:9092,b-2.kafka.com:9092,b-3.kafka.com:9092"
    enable:
      idempotence: false
    queue:
      buffering:
        max:
          messages: 1000000
          kbytes: 1048576
          ms: 1000
    message:
      send:
        max:
          retries: 2
      timeout:
        ms: 300000
    compression:
      codec: 'snappy'
    delivery:
      report:
        only:
          error: true
    request:
      required:
        acks: 1
  # Number of replicas per topic (created for each measurement)
  replicas: 3
  # Number of partitions per topic (created for each measurement)
  partitions: 1
  # time in ms for how long the topics data is retained in memory
  retention: 43200000
  # time in ms for how long the topics data is retained on the disk
  segment: 43200000

schema_creator:
  # number of workers used by the schema_creator to periodically create the schema
  num_workers: 1
  cleanup_on_boot: false
  druid_topic_prefix: "anomalydb_druid"

Druid:
  connection_string:
    overlord_ip: "anomalydb-druid-master.com"
    overlord_port: 8090
    # Connect to router via ELB
    router_ip: "internal-anomalydb-router.com"
    router_port: 80
    cc_router_ip: "internal-anomalydb-cc-router.com"
    cc_router_port: 80
    read_path: '/druid/v2/sql/'
    scheme: 'http'
    # Interval in seconds after which druidadmin reloads the full schema in
    # memory
    schema_refresh_time: 600
  datasource:
    maxRowsInMemory: 600000
    maxRowsPerSegment: 40000000
    # Drop messages that are older than 1 month. Since our regular datasources
    # only retain data for the last month, it doesn't make sense to store older
    # datapoints only for them to get deleted immediately.
    # NOTE: This will affect any historical metrics usecases
    lateMessageRejectionPeriod: P30D
    # Drop messages which are too early. Ideally this should never happen
    earlyMessageRejectionPeriod: P2D
  batch_ingest:
    # From https://druid.apache.org/docs/latest/ingestion/native-batch.html,
    # If this value is set to 1, the supervisor task processes data ingestion on
    # its own instead of spawning worker tasks.
    maxNumConcurrentSubTasks: 2


# Graphite metric consumer which stores them in flat format
metric_consumer:
  kafka_brokers: "b-1.kafka.com:9092,b-2.kafka.com:9092,b-3.kafka.com:9092"
  # `master_line` Statsrelay topic
  topic: "prod_no_agg-line-metrics"
  group_prefix: "anomalydb_metric_consumers"
  num_consumers: 1
  metric_protocol: graphite
  # We flush our async writes every 60 secs
  flush_writes_period_secs: 60
  # Hulk cluster UUID
  # whitelist_cluster_uuid: "cluster-uuid-x-y-z"
  whitelist_cluster_uuid: ""
  whitelist_node_id: ""
  # Messages with timestamp older than this will be reported through clock skew
  # turning on debug logging will log all such messages
  past_message_lag: 1800
  # Messages with timestamp newer than this will be reported through clock skew
  # turning on debug logging will log all such messages
  future_message_lag: 1800

  redis_host: "anomalydb-redis.com"
  redis_port: 6379

recovery_consumer:
  num_consumers: 1

logging:
  # Set to true (recommended for containers) to only log to console, false if log to file
  log_to_console: true
  # Log per metric level details (Extremely verbose, meant only for debugging)
  log_metrics: false
  # must be specified when log_to_console is false
  # log_dir: "/mnt/anomalydb/log"
  debug: false

telemetry:
  protocol: influx
  # We add the environment tag to all metrics
  environment: production
  influx:
    # Interval in seconds at which frequency the metrics are pushed to graphite
    reporting_interval: 600
    prefix: "anomalydb"
    database: "anomalydb-metrics"
    server: "internal.timestream.com"
    port: 80
