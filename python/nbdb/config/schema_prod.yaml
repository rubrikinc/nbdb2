# This documents the schema supported by AnomalyDB
# A measurement must be declared in the schema file, otherwise it will get dropped
#
# <measurement name>:
#  <f name>: [optional]
#    delta_threshold: [optional, default is 0.05]
#
# Minimum interval at storage stores, so all data points are aligned to ceiling of this interval
MIN_STORAGE_INTERVAL: 600

sparse_algos:
  - algo: "SparseAlgoLossLess"
    pattern:
      - ".*count$"
      - ".*uptime.*"
      - ".*\\|tkn0=internal\\|tkn1=monitoring\\|tkn2=test\\|"
  - algo: "SparseAlgoPercentileWindowDelta"
    pattern: [".*p\\d+$"]
    min_delta: 0.01
    # If possible, quantile should be chosen such that quantile*window_size is a whole number
    # This is because we don't interpolate values when computing quantile
    quantile: .9
    window_size: 10
  - algo: "SparseAlgoLastValueDelta"
    min_delta: 0.01


transforms:
  non_negative_derivative:
    include_regex: [".*service.*Run.*count$"]
    exclude_regex: []
    suffix: "t_meter"

granularities:
  query: 600
  # Default segment granularity is DAILY segments
  default_segment: DAY

# Measurement list required by influx explorer
measurements:

sparseness_disabled_patterns:
  # SupportTunnel metrics come in at 1m granularity. We want to disable
  # sparseness so that they pass right through and are not subjected to the 10m
  # granularity for other metrics
  - pattern:
      tkn3: Diamond
      tkn4: service
      tkn5: '*DB'
      field: m_2
    interval: 1200

# Patterns for deciding which metrics will be duplicated to the cross-cluster
# datasources
cross_cluster_patterns:
  # Dashboard cross-cluster queries
  - pattern: service_1.Fileset.*
    hashkey: tkn3=service_1|tkn4=Fileset
  - pattern: Diamond.process_1.Node.Status.*
    hashkey: tkn3=Diamond|tkn4=process_1|tkn5=Node|tkn6=Status
  - pattern: service_3.process_2.task_1.Errors.count
    hashkey: tkn3=service_3|tkn4=process_2
  - pattern: service_3.process_2.task_1.Errors.x*
    hashkey: tkn3=service_3|tkn4=process_2

# Data sources created in druid.
#
# Each datasource can have the following attributes:
# a. num_shards (optional) - Number of sharded Druid datasources to create. We
# will create a total of `num_shards` Druid datasources with the naming
# convention "<name>_t_<shard_suffix>"
#
# b. shard_tag_key (required if using num_shards) - We will use the value
# corresponding to the specified tag key to decide the shard suffix.
#
# c. windows (optional) - Used for rollup datasources to specify the rollup
# windows. Specify rollup window duration in seconds (eg. 3600 for 1h rollups)
datasources:
  # Datasource for flat Graphite metrics
  all_metrics:
    num_shards: 400
    # Sharded based on cluster ID which is the second token for flat Graphite
    # metrics
    shard_tag_key: tkn1

  cc:
    num_shards: 20
    shard_tag_key: none

  rollup_cc:
    num_shards: 20
    shard_tag_key: none
    windows: [3600, 14400]

  internal_metrics:

  rollup_internal_metrics:
    windows: [3600, 14400]

  # Datasource for tagged Influx metrics
  tagged_metrics:
    num_shards: 100
    # Sharded based on the Cluster tag
    shard_tag_key: Cluster

  # Rollup datasource for flat Graphite metrics
  rollup_data_source:
    num_shards: 30
    # Sharded based on cluster ID which is the second token for flat Graphite
    # metrics
    shard_tag_key: tkn1
    windows: [3600, 14400]

  # Rollup datasource for Influx metrics
  rollup_tagged_data_source:
    num_shards: 30
    # Sharded based on the Cluster tag
    shard_tag_key: Cluster
    windows: [3600, 14400]

  # Catch all rollup datasource
  rollup_catchall:
    windows: [3600, 14400]

  dashboard_queries_1:
    num_shards: 20
    # Sharded based on cluster ID which is the second token for flat Graphite
    # metrics
    shard_tag_key: tkn1

  # Datasource used for exploring tagged metrics
  exploration_tagged:
  exploration_graphite:

# Lists the datasources to be used for exploration. Indexed by protocol
# If nothing is listed for the given protocol, we will do nothing
exploration_datasources:
  influx: exploration_tagged
  graphite: exploration_graphite

# The longest prefix match found below identifies the datasource for a
# measurement. It is a many-to-one relationship, multiple prefixes can map to
# same datasource.
#
# For graphite, there is no measurement. Therefore we count the metric name
# from the fourth token onwards as the measurement. Eg. The measurement for
# 'clusters.ABC.XYZ.Diamond.uptime.seconds' would be 'Diamond.uptime.seconds'
measurement_to_datasource_mapping:
  # Prefix rules and fallback datasource for Graphite metrics
  graphite:
    rules:
      internal: internal_metrics
    # Global rule for any data source that doesnt match the prefix rules
    catch_all_datasource: all_metrics
  # Prefix rules and fallback datasource for Influx metrics
  influx:
    rules: {}
    # Global rule for any data source that doesnt match the prefix rules
    catch_all_datasource: tagged_metrics

rollups:
  # Rollup windows for all datasources
  # Window of 3600, means all points within 1 hr (aligned to epoch 0 at hourly
  # boundary) are aggregated
  windows: [3600, 14400]

  # Rollup data source mapping:
  # We iterate over each mapping rule below to determine the rollup datasource
  # to be used, and try to find the first matching rule.
  #
  # Each rule has 4 attributes:
  # a. match_tag_key: Match rule if metric contains the specified tag key
  # b. match_tag_value (optional): Match rule if metric tag value for the
  # `match_tag_key` matches the specified value.
  # c. datasource: Rollup datasource to be used
  #
  # Rule matching works in 2 ways:
  # a. Rule contains only `match_tag_key`: A metric will match this rule only
  # if it contains the tag key specified.
  # b. Rule contains both `match_tag_key` and `match_tag_value`. A metric will
  # match this rule if it matches both the tag key & value specified in the
  # rule.
  #
  # If a metric matches multiple rules, the earlier rule is given preference.
  rollup_datasource_match_rules:
    # All Graphite metrics beginning with "clusters" go to this sharded
    # datasource.
    # NOTE: The tag_key prefix should match TOKEN_TAG_PREFIX in data_point.py
    - match_tag_key: tkn0
      match_tag_value: clusters
      datasource: rollup_data_source

    - match_tag_key: tkn0
      match_tag_value: internal
      datasource: rollup_internal_metrics

    # All Influx metrics will have a tag called measurement
    - match_tag_key: measurement
      datasource: rollup_tagged_data_source

  # All metrics which do not match any of the rules above go to this fallback
  # datasource
  catch_all_datasource: rollup_catchall

  # Default rollup algorithm to use
  default_algo: "mean"
  # Regex rules to match the rollup algorithm
  algos:
    # For monotonically increasing counters, using last() makes the most sense
    last:
      - ".*count$"
      - ".*tkn3=Diamond\\|tkn4=schedulerstats\\|"
      - ".*tkn3=Service\\|tkn4=CPU\\|"
    # Diamond.nfacct metrics are like meters, requiring us to use sum() to
    # compute the right rollup values
    sum:
      - ".*tkn3=Diamond\\|tkn4=nfacct\\|"


# dashboard queries, in future this list will be runtime generated and likely stored in druid itself
dashboard_queries:
  clusters:
    - "cluster-uuid-x-y-z"
  dashboard_queries_1:
    - tkn3: ["Diamond"]
      tkn4: ["service_1"]
      tkn5: ["process_1"]
      tkn6: ["task_1"]
      tkns: ["8"]
      field: ["field_1"]
    - tkn3: ["service_2"]
      tkn4: ["process_2"]
      tkns: ["6"]
      field: ["field_2"]
    - tkn3: ["Diamond"]
      tkn4: ["process_3", "process_4"]
      tkns: ["6"]
      field: ["field_3", "field_4", "field_5"]
