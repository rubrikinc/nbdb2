"""
TestMetricKey
"""
import cProfile
import os
import time
from unittest import TestCase

from nbdb.common.context import Context
from nbdb.common.data_point import TOKEN_TAG_PREFIX, TOKEN_COUNT, MEASUREMENT
from nbdb.common.data_point import TRANSLATED_MINUS_ONE_VALUE
from nbdb.common.data_point import TRANSLATED_MINUS_THREE_VALUE
from nbdb.common.metric_parsers import MetricParsers
from nbdb.common.telemetry import Telemetry
from nbdb.common.tracing_config import TracingConfig
from nbdb.config.settings import Settings
from nbdb.schema.schema import Schema


class TestMetricParser(TestCase):
    """
    Unit tests for parsing the metric serialized strings to MetricValues
    """

    def setUp(self) -> None:
        schema = Schema.load_from_file(os.path.dirname(__file__) +
                              '/test_schema.yaml')
        self.context = Context(schema=schema)
        Settings.load_yaml_settings(os.path.dirname(__file__) +
                                    '/test_settings.yaml')
        if TracingConfig.inst is None:
            TracingConfig.initialize(Settings.inst.tracing_config)
        if Telemetry.inst is None:
            Telemetry.initialize()
        self.metric_parsers = MetricParsers(self.context, 'influx')

    def test_parse_real_influx_metric_values(self) -> None:
        """
        Test parsing of a influx line metric generated by telegraph
        sample from Turbo team performance benchmarks
        """
        msg = 'procstat,group_id=abhishek-kumar,host=$HOSTNAME,pattern=.,' \
              'process_name=ksoftirqd/6,Cluster=c0' \
              ' write_count=0i,read_count=10i ' \
              '1582326360000000000\n'

        data_points = self.metric_parsers.parse_influx_metric_values(msg)
        self.assertEqual(3, len(data_points))
        self.assertEqual(0, data_points[0].value)
        self.assertEqual(0, data_points[1].value)
        self.assertEqual(10, data_points[2].value)
        self.assertEqual(1582326360, data_points[0].epoch)
        self.assertEqual(1582326360, data_points[1].epoch)
        self.assertEqual(1582326360, data_points[2].epoch)
        self.assertEqual('procstat', data_points[0].tags[MEASUREMENT])
        self.assertEqual('ksoftirqd/6', data_points[0].tags['process_name'])
        self.assertEqual('abhishek-kumar', data_points[0].tags['group_id'])
        self.assertEqual('$HOSTNAME', data_points[0].tags['host'])
        self.assertEqual('.', data_points[0].tags['pattern'])
        self.assertEqual('procstat', data_points[1].tags[MEASUREMENT])
        self.assertEqual('ksoftirqd/6', data_points[1].tags['process_name'])
        self.assertEqual('abhishek-kumar', data_points[1].tags['group_id'])
        self.assertEqual('$HOSTNAME', data_points[1].tags['host'])
        self.assertEqual('.', data_points[1].tags['pattern'])
        self.assertEqual('procstat', data_points[2].tags[MEASUREMENT])
        self.assertEqual('ksoftirqd/6', data_points[2].tags['process_name'])
        self.assertEqual('abhishek-kumar', data_points[2].tags['group_id'])
        self.assertEqual('$HOSTNAME', data_points[2].tags['host'])
        self.assertEqual('.', data_points[2].tags['pattern'])
        self.assertEqual('graphite_flat', data_points[0].datasource)
        self.assertEqual(
            Schema.template_to_datasource('dashboard_queries_tagged', '0'),
            data_points[1].datasource)
        self.assertEqual('graphite_flat', data_points[2].datasource)
        self.assertEqual('write_count', data_points[0].field)
        self.assertEqual('write_count', data_points[1].field)
        self.assertEqual('read_count', data_points[2].field)

    def test_parse_influx_metric_values(self) -> None:
        """
        Tests the parsing logic for influx line protocol
        :return:
        """
        msg = 'measurement,k1=v1,k2=v2 field=10,field2=20 100000000000'
        data_points = self.metric_parsers.parse_influx_metric_values(msg)
        self.assertEqual(len(data_points), 2, 'Line protocol has 2 metrics')

        # Verify the parsed values are mapped to the right series
        for data_point in data_points:
            self.assertEqual(data_point.epoch, 100)
            if data_point.field == 'field':
                exp_value = 10
            else:
                exp_value = 20
            self.assertEqual(exp_value, data_point.value)

    def test_parse_invalid_influx_line(self) -> None:
        """
        Tests for the poorly formatted influx line
        :return:
        """
        for line in [
                'abc',        # Line without field and epoch
                'a,d=f 100',  # Line without fields
                'a f=2'       # Line without epoch
        ]:
            self.assertRaises(SyntaxError,
                              self.metric_parsers.parse_influx_metric_values,
                              line)

    def test_valid_influx_lines(self) -> None:
        """
        Test that we can parse valid influx lines
        :return:
        """
        for line in ['abc f=1 100000000000', 'a,d=e f=2,f1=3 100000000000']:
            self.metric_parsers.parse_influx_metric_values(line)

    def test_parse_graphite_metric_values(self) -> None:
        """
        Tests the parsing logic for graphite protocol
        :return:
        """
        # TEST1: Try parsing a CDM metric
        msg = 'clusters.ABC.RVM123.Diamond.uptime.seconds 1 100\n'
        data_points = self.metric_parsers.parse_graphite_metric_values(msg)
        self.assertEqual(len(data_points), 1, 'Graphite protocol has 1 metric')

        # Verify the parsed values are mapped to the right series
        # We should additionally also generate the cluster & node tags
        data_point = data_points[0]
        self.assertEqual(data_point.datasource, 'Diamond')
        self.assertEqual(data_point.tags[TOKEN_TAG_PREFIX + '1'], 'ABC')
        self.assertEqual(data_point.tags[TOKEN_TAG_PREFIX + '2'], 'RVM123')
        self.assertEqual(data_point.tags[TOKEN_TAG_PREFIX + '4'], 'uptime')
        self.assertEqual(data_point.field, 'seconds')
        self.assertEqual(data_point.epoch, 100)
        self.assertEqual(data_point.value, 1)

        # Make sure the tags are generated with the right values
        # TEST2: Try parsing a non-CDM metric
        msg = 'random.alerts.clusters.Bhanu.count 12 15568135610\n'
        data_points = self.metric_parsers.parse_graphite_metric_values(msg)
        self.assertEqual(len(data_points), 1, 'Graphite protocol has 1 metric')

        # Verify the parsed values are mapped to the right series
        data_point = data_points[0]
        self.assertEqual(data_point.datasource, 'graphite_flat')
        self.assertEqual(data_point.tags[TOKEN_TAG_PREFIX + '1'], 'alerts')
        self.assertEqual(data_point.tags[TOKEN_TAG_PREFIX + '2'], 'clusters')
        self.assertEqual(data_point.tags[TOKEN_TAG_PREFIX + '3'], 'Bhanu')
        self.assertEqual('count', data_point.field)
        self.assertEqual(15568135610, data_point.epoch)
        self.assertEqual(12, data_point.value)
        self.assertEqual(5, len(data_point.tags))

        # Make sure the tags are generated with the right values
        # TEST3: Try parsing a internal metric
        msg = 'internal.alerts.clusters.Bhanu.count 12 15568135610\n'
        data_points = self.metric_parsers.parse_graphite_metric_values(msg)
        self.assertEqual(len(data_points), 1, 'Graphite protocol has 1 metric')

        # Verify the parsed values are mapped to the right series
        data_point = data_points[0]
        self.assertEqual(data_point.datasource, 'internal_metrics')
        self.assertEqual(data_point.tags[TOKEN_TAG_PREFIX + '1'], 'alerts')
        self.assertEqual(data_point.tags[TOKEN_TAG_PREFIX + '2'], 'clusters')
        self.assertEqual(data_point.tags[TOKEN_TAG_PREFIX + '3'], 'Bhanu')
        self.assertEqual('count', data_point.field)
        self.assertEqual(15568135610, data_point.epoch)
        self.assertEqual(12, data_point.value)
        self.assertEqual(5, len(data_point.tags))

    def test_parse_invalid_graphite_line(self) -> None:
        """
        Tests for the poorly formatted graphite line
        :return:
        """
        for line in [
                'abc',                    # Line without value and epoch
                'abc 1.0',                # Line without epoch
                'abc 15568135610'         # Line without value
                'abc def 15568135610',    # Unparseable value
                'abc 1 def',              # Unparseable epoch
                'abc def=1 15568135610',  # Influx format
        ]:
            self.assertRaises(SyntaxError,
                              self.metric_parsers.parse_graphite_metric_values,
                              line)

    def test_valid_graphite_lines(self) -> None:
        """
        Test that we can parse valid graphite lines
        :return:
        """
        for line in ['abc.def 1.0 100', 'abc.def.efg 1 15568135610']:
            self.metric_parsers.parse_graphite_metric_values(line)

    def test_parse_graphite_flat_metric_values(self) -> None:
        """
        Tests the parsing logic for graphite flat protocol
        :return:
        """
        # TEST1: Try parsing a CDM metric
        msg = 'clusters.ABC.RVM123.Diamond.uptime.seconds 1 100\n'
        data_points = self.metric_parsers.parse_graphite_metric_values(msg)
        self.assertEqual(len(data_points), 1, 'Graphite protocol has 1 metric')

        # Verify that we decoded the measurement, field & values correctly
        data_point = data_points[0]
        self.assertEqual('Diamond', data_point.datasource)
        self.assertEqual('6', data_point.tags[TOKEN_COUNT])
        for i in range(5):
            self.assertTrue(TOKEN_TAG_PREFIX + str(i) in data_point.tags)
        self.assertEqual('seconds',
                         data_point.field)
        self.assertEqual(data_point.epoch, 100)
        self.assertEqual(data_point.value, 1)

    def test_parse_dashboard_data_point_metric_values(self) -> None:
        """
        Tests the parsing logic for graphite flat protocol
        :return:
        """
        # TEST1: Try parsing a CDM metric
        msg = 'clusters.cluster-uuid-x-y-z.n_0.' \
              'sharded_data_source.f_0.c_1.n_0.count 1586 1586299980\n'
        data_points = self.metric_parsers.parse_graphite_metric_values(msg)
        self.assertEqual(len(data_points), 2, 'Expected dashboard datapoint')

        # Verify that we decoded the measurement, field & values correctly
        data_point = data_points[0]
        self.assertEqual('graphite_flat', data_point.datasource)
        self.assertEqual('8', data_point.tags[TOKEN_COUNT])
        for i in range(7):
            self.assertTrue(TOKEN_TAG_PREFIX + str(i) in data_point.tags)
        self.assertEqual('count',
                         data_point.field)
        self.assertEqual(data_point.epoch, 1586299980)
        self.assertEqual(data_point.value, 1586)

        data_point = data_points[1]
        self.assertEqual('dashboard_queries_t_0', data_point.datasource)
        self.assertEqual('8', data_point.tags[TOKEN_COUNT])
        for i in range(7):
            self.assertTrue(TOKEN_TAG_PREFIX + str(i) in data_point.tags)
        self.assertEqual('count',
                         data_point.field)
        self.assertEqual(data_point.epoch, 1586299980)
        self.assertEqual(data_point.value, 1586)

    @staticmethod
    def _generate_messages(num_clusters, num_nodes):
        """
        Generate dummy data
        :param num_clusters:
        :param num_nodes:
        :return:
        """
        msgs = list()
        for c in range(0, num_clusters):
            for n in range(0, num_nodes):
                msg = 'Diamond.process,' \
                      'Cluster=f5888c22-9651-4cd0-8e3a-90367d9242c{},' \
                      'Node=RVM123{},' \
                      'major_version=5,' \
                      'minor_version=1,' \
                      'patch=1254,' \
                      'hardware=RVM,' \
                      'edge=true' \
                      ' cpu_perc=12,mem=100,net_recv=19312,net_trans=313131' \
                      ' {}'.\
                    format(c, n, time.time_ns())
                msgs.append(msg)
        return msgs

    def test_dashboard_series(self) -> None:
        """
        Tests for dashboard series
        :return:
        """
        msg = 'clusters.ABC.RVM123.Diamond.uptime.count 1 100\n'
        data_points = self.metric_parsers.parse_graphite_metric_values(msg)
        self.assertEqual(2, len(data_points), 'Expected dashboard metric')
        msg = 'clusters.ABC.RVM123.Process.count 1 100\n'
        data_points = self.metric_parsers.parse_graphite_metric_values(msg)
        self.assertEqual(2, len(data_points), 'Expected dashboard metric')
        msg = 'clusters.ABC.RVM123.Process.Influx.count 1 100\n'
        data_points = self.metric_parsers.parse_graphite_metric_values(msg)
        self.assertEqual(1, len(data_points), 'Expected dashboard metric')

    def test_datapoints_using_reserved_values(self) -> None:
        """
        Tests whether we correctly convert datapoints using reserved values -1
        and -3 to some other values
        :return:
        """
        # TEST1: Datapoint using -1
        msg = 'clusters.ABC.RVM123.Diamond.uptime.seconds -1 100\n'
        data_points = self.metric_parsers.parse_graphite_metric_values(msg)

        self.assertEqual(len(data_points), 1, 'Graphite protocol has 1 metric')
        data_point = data_points[0]
        self.assertEqual(data_point.epoch, 100)
        self.assertEqual(data_point.value, TRANSLATED_MINUS_ONE_VALUE)
        self.assertTrue(str(TRANSLATED_MINUS_ONE_VALUE) in
                        data_point.to_druid_json_str())

        # TEST2: Datapoint using -3
        msg = 'clusters.ABC.RVM123.Diamond.uptime.seconds -3 100\n'
        data_points = self.metric_parsers.parse_graphite_metric_values(msg)

        self.assertEqual(len(data_points), 1, 'Graphite protocol has 1 metric')
        data_point = data_points[0]
        self.assertEqual(data_point.epoch, 100)
        self.assertEqual(data_point.value, TRANSLATED_MINUS_THREE_VALUE)
        self.assertTrue(str(TRANSLATED_MINUS_THREE_VALUE) in
                        data_point.to_druid_json_str())

    def test_metric_parser_perf(self) -> None:
        """
        Test the performance of metric parser
        """
        msgs = TestMetricParser._generate_messages(100, 100)
        enable_profile = False
        pr = cProfile.Profile()
        if enable_profile:
            pr.enable()
        t_proc_time = time.time()
        for msg in msgs:
            self.metric_parsers.parse_influx_metric_values(msg)
        t_proc_time = time.time() - t_proc_time
        if enable_profile:
            pr.disable()
            pr.print_stats(sort="tottime")
        throughput = int(len(msgs)/t_proc_time)
        reference_throughput = 10000
        performance_drop = int(100 - throughput * 100 / reference_throughput)
        print('throughput: {} msgs/s perf drop: {} %'.format(throughput,
                                                             performance_drop))
        last_profile =\
            "\n____________________________________________________________"\
            "\nncalls  tottime  percall  cumtime  percall " \
            "filename:lineno(function)"\
            "\n3000    0.029    0.000    0.107    0.000 " \
            "metric_parsers.py:17(parse_influx_metric_values)"\
             "\n6000    0.021    0.000    0.033    0.000 " \
            "metric_parsers.py:57(_parse_key_value_pairs)"\
            "\n12000    0.021    0.000    0.036    0.000 " \
            "data_point.py:18(__init__)"\
            "\n42000    0.014    0.000    0.014    0.000 " \
            "{method 'split' of 'str' objects}"\
            "\n12000    0.009    0.000    0.009    0.000 " \
            "{built-in method builtins.sorted}"\
            "\n12000    0.004    0.000    0.004    0.000 " \
            "{method 'join' of 'str' objects}"\
            "\n36000    0.003    0.000    0.003    0.000 " \
            "{built-in method builtins.len}"\
            "\n12000    0.002    0.000    0.002    0.000 " \
            "{method 'strip' of 'str' objects}"\
            "\n12000    0.002    0.000    0.002    0.000 " \
            "{method 'values' of 'dict' objects}"\
            "\n12000    0.002    0.000    0.002    0.000 " \
            "{method 'append' of 'list' objects}"\
            "\n    2    0.000    0.000    0.000    0.000 " \
            "{built-in method time.time}"\
            "\n     1    0.000    0.000    0.000    0.000 " \
            "{method 'disable' of '_lsprof.Profiler' objects}"\
            "\n____________________________________________________________"
        self.assertGreater(20,
                           performance_drop,
                           '\n MetricParser performance has dropped by {} %'
                           '\n Make sure your CPU usage is not high'
                           '\n Please set enable_profile=True and test again'
                           '\n For reference the following profile had'
                           ' throughput of {} K msgs/s'
                           '\n compared to {} K msgs/s you got. {}'.format(
                               performance_drop,
                               reference_throughput,
                               throughput,
                               last_profile
                               ))
